{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4845, -0.2887,  2.1277], requires_grad=True)\n",
      "tensor([0.5155, 1.7113, 4.1277], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000002BF2D571D50>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Autograd \n",
    " -> automatic differentiation for all operations on Tensors\n",
    " -> engine for computing the vector-Jacobian product.\n",
    " -> compute partial derivates while applying the chain rule\n",
    " -> set requires_grad = True\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# require_grad = True -> tracks all operations on the tensor.\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor.\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7972,  8.7855, 51.1146], grad_fn=<MulBackward0>)\n",
      "tensor(20.2324, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Do more operations on y\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([1.0310, 3.4226, 8.2555])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's compute the gradients with backpropagation\n",
    "When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "The gradient for this tensor will be accumulated into .grad attribute.\n",
    "It is partial derivate of the function w.r.t. the tensor\n",
    "\"\"\"\n",
    "\n",
    "print(x.grad)\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "\"\"\"\n",
    "!!!Careful!!! backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "we need to set the gradients to zero -> when we for loop for each epoch\n",
    "!!!We need to be careful during optimization !!! optimizer.zero_grad() -> set gradient to zero\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop a tensor from tracking history\n",
    "\"\"\"\n",
    "Stop a tensor from tracking history\n",
    " -> during the training loop when we want to update our weights\n",
    " -> after training during evaluation\n",
    " \n",
    " these operations should not be part of the gradient computation. To prevent this we can use:\n",
    "  * x.requires_grad_(False)\n",
    "  * x.detach()\n",
    "  * warp in -> with torch.no_grad():\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "\n",
      "True\n",
      "<SumBackward0 object at 0x000002BF2D918F10>\n",
      "\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_(...) changes an existing flag in-place.\n",
    "a = torch.randn(2,2)\n",
    "b = (a * a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)\n",
    "\n",
    "print()\n",
    "\n",
    "a.requires_grad_(True)\n",
    "b = (a * a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)\n",
    "print()\n",
    "\n",
    "a.requires_grad_(False)\n",
    "b = (a * a).sum()\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5631, -0.9173],\n",
      "        [-0.1732, -0.9280]], requires_grad=True)\n",
      "True\n",
      "tensor([[-1.5631, -0.9173],\n",
      "        [-0.1732, -0.9280]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# .detach(): get a new Tensor with the same content but no gradient computation:\n",
    "a = torch.randn(2,2, requires_grad=True)\n",
    "b = a.detach()\n",
    "\n",
    "print(a)\n",
    "print(a.requires_grad)\n",
    "\n",
    "print(b)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7098, -0.2711],\n",
      "        [-0.6618, -1.2170]], requires_grad=True)\n",
      "tensor([[0.5039, 0.0735],\n",
      "        [0.4380, 1.4810]])\n",
      "tensor([[0.5039, 0.0735],\n",
      "        [0.4380, 1.4810]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# wrap in \"with torch.no_grad():\"\n",
    "a = torch.randn(2,2,requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "# print(a.grad)\n",
    "# print(a)\n",
    "\n",
    "with torch.no_grad():\n",
    "    b = a**2\n",
    "    print(b.requires_grad)\n",
    "    \n",
    "# print(a.grad)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1650, -1.1990,  0.4117],\n",
      "        [-1.5180,  0.1664,  0.3169],\n",
      "        [ 1.5767,  0.0829,  1.3169]], requires_grad=True)\n",
      "tensor([[0.0272, 1.4375, 0.1695],\n",
      "        [2.3043, 0.0277, 0.1004],\n",
      "        [2.4860, 0.0069, 1.7342]])\n",
      "tensor([[0.0272, 1.4375, 0.1695],\n",
      "        [2.3043, 0.0277, 0.1004],\n",
      "        [2.4860, 0.0069, 1.7342]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.randn(3,3,requires_grad=True)\n",
    "print(c)\n",
    "with torch.no_grad():\n",
    "    c = c **2\n",
    "    print(c)\n",
    "print(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: tensor([[-0.4490]], requires_grad=True)\n",
      "x: tensor([[-0.8980]], grad_fn=<MulBackward0>)\n",
      "c grad: tensor([[2.]])\n",
      "x: tensor([[-0.8980]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = torch.randn(1,1,requires_grad=True)\n",
    "print(\"c:\",c)\n",
    "x = c*2\n",
    "print(\"x:\",x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c = c **2\n",
    "    c.requires_grad_(True)\n",
    "    print(c)\n",
    "\n",
    "print(\"c:\",c)\n",
    "print(\"x:\",x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
